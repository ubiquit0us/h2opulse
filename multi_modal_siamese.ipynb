{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClC47lq54XjF"
      },
      "source": [
        "**Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV6J8Q_R2CMN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from os import path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhT4YhFm4WhV"
      },
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "def load_config(config_path='config_siamese.yaml'):\n",
        "    with open(config_path, 'r') as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    return config\n",
        "\n",
        "config = load_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuRQtc7Jk_m8",
        "outputId": "64476667-1d23-41dd-e7af-ab10ffa4898a"
      },
      "outputs": [],
      "source": [
        "epochs = config['hyperparameters']['epochs']\n",
        "batch_size = config['hyperparameters']['batch_size']\n",
        "margin = config['hyperparameters']['margin']\n",
        "\n",
        "dataset_path = config['dataset']['base_path']\n",
        "dataset_version = config['dataset']['version']\n",
        "roi_folder_name = config['dataset']['roi_folder_name']\n",
        "number_of_roi_per_user = config['dataset']['num_roi']\n",
        "landmarks = config['dataset']['landmarks']\n",
        "\n",
        "# Example usage\n",
        "print(f\"Training for {epochs} epochs with batch size {batch_size} and margin {margin}\")\n",
        "print(f\"Dataset path: {dataset_path}{dataset_version}, ROI folder: {roi_folder_name}\")\n",
        "\n",
        "\n",
        "users = os.listdir(base_dataset_path + dataset_version)\n",
        "random.shuffle(users)\n",
        "length = len(users)\n",
        "training_len = (int)(length*0.9)\n",
        "users_train_val = users[:training_len]\n",
        "users_test = users[training_len:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GavZA-VYxQjy"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "def list_files_in_directory(directory):\n",
        "  files = []\n",
        "  for file in os.listdir(directory):\n",
        "    # Join the directory path with the file name to get the full path\n",
        "    full_path = os.path.join(directory, file)\n",
        "    if os.path.isfile(full_path):\n",
        "      files.append(full_path)\n",
        "  return files\n",
        "\n",
        "def get_file_path_list(dir):\n",
        "  # Get a list of all files in the current directory\n",
        "  all_files = list_files_in_directory(dir)\n",
        "  if len(all_files) < number_of_roi_per_user:\n",
        "    first_file = all_files[0]\n",
        "    while(len(all_files) < number_of_roi_per_user):\n",
        "      all_files.append(first_file)\n",
        "  # Get a list of all file paths in the current directory\n",
        "  all_file_paths = [os.path.join(dir, file) for file in all_files]\n",
        "\n",
        "\n",
        "  return all_file_paths[:number_of_roi_per_user]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZmHGo76Bk78"
      },
      "outputs": [],
      "source": [
        "def create_triplets(landmark, hand, users):\n",
        "  triplets = pd.DataFrame(columns=['anchor_image', 'hydrated_image', 'dehydrated_image'])\n",
        "\n",
        "  index = 0\n",
        "\n",
        "  for user in users:\n",
        "    hydrated_list_url = base_dataset_path + dataset_version + user + \"/\" + \"hydrated/\" + hand + \"/\" + landmark + \"/\" + roi_folder_name\n",
        "    hydrated_list = get_file_path_list(hydrated_list_url)\n",
        "\n",
        "    dehydrated_list_url = base_dataset_path + dataset_version + user + \"/\" + \"dehydrated/\" + hand + \"/\" + landmark + \"/\" + roi_folder_name\n",
        "    dehydrated_list = get_file_path_list(dehydrated_list_url)\n",
        "\n",
        "    if ((len(hydrated_list) == 0) or (len(dehydrated_list) == 0)):\n",
        "      continue\n",
        "\n",
        "    anchor = hydrated_list[0]\n",
        "\n",
        "    for i in range(len(dehydrated_list)):\n",
        "      dehydrated = dehydrated_list[i]\n",
        "      for j in range(len(hydrated_list)):\n",
        "        hydrated = hydrated_list[j]\n",
        "        triplets.loc[len(triplets)] = [anchor, hydrated, dehydrated]\n",
        "  return triplets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtL8MghTKjOt"
      },
      "outputs": [],
      "source": [
        "def load_images(triplets):\n",
        "  anchor_images_array = []\n",
        "  hydrated_images_array = []\n",
        "  dehydrated_images_array = []\n",
        "\n",
        "  count = 0\n",
        "  fileError = 0\n",
        "\n",
        "  for i, directory in triplets['anchor_image'].items():\n",
        "    try:\n",
        "      img = image.load_img(directory, target_size=(200, 200))\n",
        "      img = image.img_to_array(img)\n",
        "      img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "      img = preprocess_input(img)\n",
        "\n",
        "      anchor_images_array.append(img)\n",
        "      count = count + 1\n",
        "    except FileNotFoundError:\n",
        "      fileError = fileError + 1\n",
        "\n",
        "  print(len(anchor_images_array))\n",
        "\n",
        "  for i, directory in triplets['hydrated_image'].items():\n",
        "    try:\n",
        "      img = image.load_img(directory, target_size=(200, 200))\n",
        "      img = image.img_to_array(img)\n",
        "      img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "      img = preprocess_input(img)\n",
        "\n",
        "      hydrated_images_array.append(img)\n",
        "      count = count + 1\n",
        "    except FileNotFoundError:\n",
        "      fileError = fileError + 1\n",
        "\n",
        "  print(len(hydrated_images_array))\n",
        "\n",
        "  for i, directory in triplets['dehydrated_image'].items():\n",
        "    try:\n",
        "      img = image.load_img(directory, target_size=(200, 200))\n",
        "      img = image.img_to_array(img)\n",
        "      img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "      img = preprocess_input(img)\n",
        "\n",
        "      dehydrated_images_array.append(img)\n",
        "      count = count + 1\n",
        "    except FileNotFoundError:\n",
        "      fileError = fileError + 1\n",
        "\n",
        "  print(len(dehydrated_images_array))\n",
        "\n",
        "  image_triplets = []\n",
        "\n",
        "  for i in range(len(anchor_images_array)):\n",
        "    anchor_img = anchor_images_array[i]\n",
        "    hydrated_img = hydrated_images_array[i]\n",
        "    dehydrated_img = dehydrated_images_array[i]\n",
        "    image_triplets += [[anchor_img, hydrated_img, dehydrated_img]]\n",
        "\n",
        "  print(len(image_triplets))\n",
        "  return image_triplets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr0s7o9tGpLe"
      },
      "outputs": [],
      "source": [
        "def make_pairs(triplets):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    for idx in range(len(triplets)):\n",
        "      # add a matching example\n",
        "      x1 = triplets[idx][0]\n",
        "      x2 = triplets[idx][1]\n",
        "      pairs += [[x1, x2]]\n",
        "      labels += [1]\n",
        "\n",
        "      # add a non-matching example\n",
        "      x1 = triplets[idx][0]\n",
        "      x2 = triplets[idx][2]\n",
        "      pairs += [[x1, x2]]\n",
        "      labels += [0]\n",
        "\n",
        "    return np.array(pairs), np.array(labels).astype(\"float32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYIndxzyODNs"
      },
      "outputs": [],
      "source": [
        "def split_dataset_pair(x, y):\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # index 0 has anchor that means x_train_1 has acnhor images only and x_train_2 hydrated or dehydarted\n",
        "  x_train_1 = x_train[:, 0]\n",
        "  x_train_2 = x_train[:, 1]\n",
        "\n",
        "  x_val_1 = x_val[:, 0]\n",
        "  x_val_2 = x_val[:, 1]\n",
        "\n",
        "  return (x_train_1, x_train_2, x_val_1, x_val_2, y_train, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE5KNHIs2DQl"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
        "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
        "\n",
        "def manhattan_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sum(K.abs(x - y), axis=1, keepdims=True)\n",
        "\n",
        "def cosine_distance(vects):\n",
        "    x, y = vects\n",
        "    x_norm = tf.nn.l2_normalize(x, axis = 1)\n",
        "    y_norm = tf.nn.l2_normalize(y, axis = 1)\n",
        "    cos = tf.math.reduce_sum(x_norm * y_norm, axis = 1, keepdims=True)\n",
        "    return tf.math.abs(cos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e81OMCE_6W-D"
      },
      "source": [
        "**Multi Modal Siamese Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qBWWWia6z1p"
      },
      "source": [
        "Embedding for dorsal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC5sXe4ykriG"
      },
      "source": [
        "left hand dorsal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74Gyn3Fo6Un5"
      },
      "outputs": [],
      "source": [
        "input = layers.Input((200, 200, 3))\n",
        "x = tf.keras.layers.BatchNormalization()(input)\n",
        "x = layers.Conv2D(4, (5, 5), activation=\"relu\")(x)\n",
        "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = layers.Conv2D(16, (5, 5), activation=\"relu\")(x)\n",
        "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = layers.Dense(10, activation=\"relu\")(x)\n",
        "embedding_network_dorsal_left = keras.Model(input, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9JOq-zfkqEe"
      },
      "source": [
        "right hand dorsal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpHOuoSvQ63r"
      },
      "outputs": [],
      "source": [
        "input2 = layers.Input((200, 200, 3))\n",
        "x2 = tf.keras.layers.BatchNormalization()(input2)\n",
        "x2 = layers.Conv2D(5, (5, 5), activation=\"relu\")(x2)\n",
        "x2 = layers.AveragePooling2D(pool_size=(2, 2))(x2)\n",
        "x2 = layers.Conv2D(20, (5, 5), activation=\"relu\")(x2)\n",
        "x2 = layers.AveragePooling2D(pool_size=(2, 2))(x2)\n",
        "x2 = layers.Flatten()(x2)\n",
        "\n",
        "x2 = tf.keras.layers.BatchNormalization()(x2)\n",
        "x2 = layers.Dense(10, activation=\"relu\")(x2)\n",
        "embedding_network_dorsal_right = keras.Model(input2, x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOFyAx4N64Fc"
      },
      "source": [
        "Embedding for wrist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_Ycnl1_Hpbi"
      },
      "source": [
        "left hand wrist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE0YTuhz6688"
      },
      "outputs": [],
      "source": [
        "input3 = layers.Input((200, 200, 3))\n",
        "x3 = tf.keras.layers.BatchNormalization()(input3)\n",
        "x3 = layers.Conv2D(5, (5, 5), activation=\"relu\")(x3)\n",
        "x3 = layers.AveragePooling2D(pool_size=(2, 2))(x3)\n",
        "x3 = layers.Conv2D(25, (5, 5), activation=\"relu\")(x3)\n",
        "x3 = layers.AveragePooling2D(pool_size=(2, 2))(x3)\n",
        "x3 = layers.Flatten()(x3)\n",
        "\n",
        "x3 = tf.keras.layers.BatchNormalization()(x3)\n",
        "x3 = layers.Dense(10, activation=\"relu\")(x3)\n",
        "embedding_network_wrist_left = keras.Model(input3, x3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmn8-StqlbSH"
      },
      "source": [
        "right hand wrist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScNYorBEla2X"
      },
      "outputs": [],
      "source": [
        "input4 = layers.Input((200, 200, 3))\n",
        "x4 = tf.keras.layers.BatchNormalization()(input4)\n",
        "x4 = layers.Conv2D(4, (5, 5), activation=\"relu\")(x4)\n",
        "x4 = layers.AveragePooling2D(pool_size=(2, 2))(x4)\n",
        "x4 = layers.Conv2D(16, (5, 5), activation=\"relu\")(x4)\n",
        "x4 = layers.AveragePooling2D(pool_size=(2, 2))(x4)\n",
        "x4 = layers.Flatten()(x4)\n",
        "\n",
        "x4 = tf.keras.layers.BatchNormalization()(x4)\n",
        "x4 = layers.Dense(10, activation=\"relu\")(x4)\n",
        "embedding_network_wrist_right = keras.Model(input4, x4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIH-l0Xf69s6"
      },
      "outputs": [],
      "source": [
        "input_left_dorsal_ref = layers.Input((200, 200, 3))\n",
        "input_left_dorsal_state = layers.Input((200, 200, 3))\n",
        "\n",
        "input_right_dorsal_ref = layers.Input((200, 200, 3))\n",
        "input_right_dorsal_state = layers.Input((200, 200, 3))\n",
        "\n",
        "input_left_wrist_ref = layers.Input((200, 200, 3))\n",
        "input_left_wrist_state = layers.Input((200, 200, 3))\n",
        "\n",
        "input_right_wrist_ref = layers.Input((200, 200, 3))\n",
        "input_right_wrist_state = layers.Input((200, 200, 3))\n",
        "\n",
        "input_refs = [input_left_dorsal_ref, input_right_dorsal_ref, input_left_wrist_ref, input_right_wrist_ref]\n",
        "input_states = [input_left_dorsal_state, input_right_dorsal_state, input_left_wrist_state, input_right_wrist_state]\n",
        "\n",
        "tower_ref_dorsal_left = embedding_network_dorsal_left(input_refs[0])\n",
        "tower_state_dorsal_left = embedding_network_dorsal_left(input_states[0])\n",
        "\n",
        "tower_ref_dorsal_right = embedding_network_dorsal_right(input_refs[1])\n",
        "tower_state_dorsal_right = embedding_network_dorsal_right(input_states[1])\n",
        "\n",
        "tower_ref_wrist_left = embedding_network_wrist_left(input_refs[2])\n",
        "tower_state_wrist_left = embedding_network_wrist_left(input_states[2])\n",
        "\n",
        "tower_ref_wrist_right = embedding_network_wrist_right(input_refs[3])\n",
        "tower_state_wrist_right = embedding_network_wrist_right(input_states[3])\n",
        "\n",
        "# tower_ref feature merge\n",
        "# number row must be same as before but feature will be merged\n",
        "tower_ref = layers.Concatenate(axis=-1)([tower_ref_dorsal_left, tower_ref_dorsal_right, tower_ref_wrist_left, tower_ref_wrist_right])\n",
        "tower_state = layers.Concatenate(axis=-1)([tower_state_dorsal_left, tower_state_dorsal_right, tower_state_wrist_left, tower_state_wrist_right])\n",
        "\n",
        "merge_layer = layers.Lambda(euclidean_distance)([tower_ref, tower_state])\n",
        "normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)\n",
        "output_layer = layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n",
        "\n",
        "# order in inputs must be maintained\n",
        "siamese = keras.Model(inputs=[input_refs, input_states], outputs=output_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-jjwIwt2DQl"
      },
      "outputs": [],
      "source": [
        "def loss(margin=1):\n",
        "    def contrastive_loss(y_true, y_pred):\n",
        "        square_pred = tf.math.square(y_pred)\n",
        "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
        "        return tf.math.reduce_mean(\n",
        "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
        "        )\n",
        "\n",
        "    return contrastive_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Floop5Yk3gmk",
        "outputId": "85f578e3-60d6-43ba-fabf-fc55860189ee"
      },
      "outputs": [],
      "source": [
        "siamese.compile(loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"])\n",
        "siamese.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zomxJUlH0UNo"
      },
      "outputs": [],
      "source": [
        "def load_pairs(users):\n",
        "  left_hand = \"left_hand\"\n",
        "  right_hand = \"right_hand\"\n",
        "\n",
        "  dorsal = 'dorsal'\n",
        "  wrist = 'wrist'\n",
        "\n",
        "  triplets_left_dorsal = create_triplets(dorsal, left_hand, users)\n",
        "  triplets_left_dorsal = triplets_left_dorsal[triplets_left_dorsal.notna()]\n",
        "\n",
        "  triplets_right_dorsal = create_triplets(dorsal, right_hand, users)\n",
        "  triplets_right_dorsal = triplets_right_dorsal[triplets_right_dorsal.notna()]\n",
        "\n",
        "  triplets_left_wrist = create_triplets(wrist, left_hand, users)\n",
        "  triplets_left_wrist = triplets_left_wrist[triplets_left_wrist.notna()]\n",
        "\n",
        "  triplets_right_wrist = create_triplets(wrist, right_hand, users)\n",
        "  triplets_right_wrist = triplets_right_wrist[triplets_right_wrist.notna()]\n",
        "\n",
        "  image_triplets_left_dorsal = load_images(triplets_left_dorsal)\n",
        "  image_triplets_right_dorsal = load_images(triplets_right_dorsal)\n",
        "\n",
        "  image_triplets_left_wrist = load_images(triplets_left_wrist)\n",
        "  image_triplets_right_wrist = load_images(triplets_right_wrist)\n",
        "\n",
        "  res_left_dorsal = make_pairs(image_triplets_left_dorsal)\n",
        "  res_right_dorsal = make_pairs(image_triplets_right_dorsal)\n",
        "\n",
        "  res_left_wrist = make_pairs(image_triplets_left_wrist)\n",
        "  res_right_wrist = make_pairs(image_triplets_right_wrist)\n",
        "\n",
        "  pairs_left_dorsal = res_left_dorsal[0]\n",
        "  labels_left_dorsal = res_left_dorsal[1]\n",
        "\n",
        "  pairs_right_dorsal = res_right_dorsal[0]\n",
        "  labels_right_dorsal = res_right_dorsal[1]\n",
        "\n",
        "  pairs_left_wrist = res_left_wrist[0]\n",
        "  labels_left_wrist = res_left_wrist[1]\n",
        "\n",
        "  pairs_right_wrist = res_right_wrist[0]\n",
        "  labels_right_wrist = res_right_wrist[1]\n",
        "\n",
        "  print(labels_left_dorsal)\n",
        "  print(labels_right_dorsal)\n",
        "  print(labels_left_wrist)\n",
        "  print(labels_right_wrist)\n",
        "\n",
        "  pairs = [pairs_left_dorsal, pairs_right_dorsal, pairs_left_wrist, pairs_right_wrist]\n",
        "  labels = labels_left_dorsal\n",
        "\n",
        "  return pairs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjn6G1B2o3t-"
      },
      "outputs": [],
      "source": [
        "def landmark_model():\n",
        "  pairs_train, labels_train = load_pairs(users_train_val)\n",
        "\n",
        "  (x_train_left_dorsal_ref, x_train_left_dorsal_state, x_val_left_dorsal_ref,\n",
        "  x_val_left_dorsal_state, y_train_left_dorsal, y_val_left_dorsal) = split_dataset_pair(pairs_train[0], labels_train)\n",
        "\n",
        "  (x_train_right_dorsal_ref, x_train_right_dorsal_state, x_val_right_dorsal_ref,\n",
        "  x_val_right_dorsal_state, y_train_right_dorsal, y_val_right_dorsal) = split_dataset_pair(pairs_train[1], labels_train)\n",
        "\n",
        "  (x_train_left_wrist_ref, x_train_left_wrist_state, x_val_left_wrist_ref,\n",
        "  x_val_left_wrist_state, y_train_left_wrist, y_val_left_wrist) = split_dataset_pair(pairs_train[2], labels_train)\n",
        "\n",
        "  (x_train_right_wrist_ref, x_train_right_wrist_state, x_val_right_wrist_ref,\n",
        "  x_val_right_wrist_state, y_train_right_wrist, y_val_right_wrist) = split_dataset_pair(pairs_train[3], labels_train)\n",
        "\n",
        "  y_train = y_train_left_dorsal\n",
        "  y_val = y_val_left_dorsal\n",
        "\n",
        "  input_train_refs = []\n",
        "  input_train_states = []\n",
        "\n",
        "  input_train_refs.append(x_train_left_dorsal_ref)\n",
        "  input_train_refs.append(x_train_right_dorsal_ref)\n",
        "  input_train_refs.append(x_train_left_wrist_ref)\n",
        "  input_train_refs.append(x_train_right_wrist_ref)\n",
        "\n",
        "  input_train_states.append(x_train_left_dorsal_state)\n",
        "  input_train_states.append(x_train_right_dorsal_state)\n",
        "  input_train_states.append(x_train_left_wrist_state)\n",
        "  input_train_states.append(x_train_right_wrist_state)\n",
        "\n",
        "  input_val_refs = []\n",
        "  input_val_states = []\n",
        "\n",
        "  input_val_refs.append(x_val_left_dorsal_ref)\n",
        "  input_val_refs.append(x_val_right_dorsal_ref)\n",
        "  input_val_refs.append(x_val_left_wrist_ref)\n",
        "  input_val_refs.append(x_val_right_wrist_ref)\n",
        "\n",
        "  input_val_states.append(x_val_left_dorsal_state)\n",
        "  input_val_states.append(x_val_right_dorsal_state)\n",
        "  input_val_states.append(x_val_left_wrist_state)\n",
        "  input_val_states.append(x_val_right_wrist_state)\n",
        "\n",
        "\n",
        "  history = siamese.fit(\n",
        "    [input_train_refs, input_train_states], y_train,\n",
        "    validation_data=([input_val_refs, input_val_states], y_val),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "  )\n",
        "\n",
        "  # load test data\n",
        "  pairs_test, labels_test = load_pairs(users_test)\n",
        "\n",
        "  x_test_left_dorsal_ref = pairs_test[0][:, 0]\n",
        "  x_test_left_dorsal_state = pairs_test[0][:, 1]\n",
        "\n",
        "  x_test_right_dorsal_ref = pairs_test[1][:, 0]\n",
        "  x_test_right_dorsal_state = pairs_test[1][:, 1]\n",
        "\n",
        "  x_test_left_wrist_ref = pairs_test[2][:, 0]\n",
        "  x_test_left_wrist_state = pairs_test[2][:, 1]\n",
        "\n",
        "  x_test_right_wrist_ref = pairs_test[3][:, 0]\n",
        "  x_test_right_wrist_state = pairs_test[3][:, 1]\n",
        "\n",
        "  input_test_refs = []\n",
        "  input_test_states = []\n",
        "\n",
        "  input_test_refs.append(x_test_left_dorsal_ref)\n",
        "  input_test_refs.append(x_test_right_dorsal_ref)\n",
        "  input_test_refs.append(x_test_left_wrist_ref)\n",
        "  input_test_refs.append(x_test_right_wrist_ref)\n",
        "\n",
        "  input_test_states.append(x_test_left_dorsal_state)\n",
        "  input_test_states.append(x_test_right_dorsal_state)\n",
        "  input_test_states.append(x_test_left_wrist_state)\n",
        "  input_test_states.append(x_test_right_wrist_state)\n",
        "\n",
        "  predictions = siamese.predict([input_test_refs, input_test_states])\n",
        "\n",
        "  return history, predictions, labels_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDVVzH2Q30fL",
        "outputId": "ed14138b-02de-4963-e5a4-4a469f9e6bb4"
      },
      "outputs": [],
      "source": [
        "# Function to calculate evaluation metrics\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    \n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    \n",
        "    return accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDjpqf3PkCqE",
        "outputId": "a2878288-e3e5-40a1-f094-247dafd19a81"
      },
      "outputs": [],
      "source": [
        "history, predictions, y_test = landmark_model()\n",
        "evaluate_model(y_test, predictions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
